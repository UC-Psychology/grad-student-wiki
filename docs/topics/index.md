# Research Topics
###### Written by Trent Wirth, 2024-01-11

Speaking into the camera here, it was brought to my attention last semester that it would be interesting to have a document on our wiki site that reflected the research topics that I (Trent), as the faculty responsible for running the research practicum, am naturally interested in. 

For now, that information will live here. As we build the documentation site this semester, I expect this file to become buried (reasonably so) so that we might focus the content on information that is useful to you all.

## Research Topics that Trent is into
### General Interests
- Self-organization
- Emergence
- Complex Systems
- Behavioral Dynamics
- Agent-based Modeling
- Vision Science
- Spatial Navigation
- Spatial Cognition
- UX Research
- Affordances of LLMs
- Affordances of Mixed Reality Spaces (AR/VR)
- Game Development
- Storytelling

### Specific Research Questions
*Maybe too specific*

**In no particular order:**

1. Can we manipulate the [momentum vector](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009575) and measure predictable variation in the locomotor system?
    - There is a small debate within ecological optics at the moment about the importance of the information that is picked up by the eyes as we walk. 
        - Warren & Hannon 1990 called eye movements a "problem" for the visual system in the service of locomotion because of the instability of the retinal image. 
        - Work by Jon Matthis and colleagues demonstrates that the movement of the eye through space generates a signal that could be used by the locomotor system to predict the future state of the body. They refer to this as the `momentum vector`.
    - Can we design an experiment that tests whether perturbing the momentum vector would have an influence on locomotor control, contrary to the prediction made by Warren & Hannon 1990, but in line with the prediction made by the "rich retinotopy view" (as I'm now calling it)?

> the bigger question in #1 is the interest to investigate the relationship between retinal stimulation and full-body motor control (locomotor, or otherwise).

2. How does personal identity map to the spaces in which we inhabit?
    - Generally speaking, I want to work towards developing a theory of Ecological Personal Identity.
        - I've been thinking about this idea since I was an undergraduate; I think I wrote a paper titled something cute like "Me, Myself, and the Living Room Furniture" /grimacing/.
    - This idea is both aspirational, in that I think developing a language around an ecological theory of personal identity would be incredibly helpful to our society, but I also see it as a research topic that needs further investigating.
    - Last semester, I started a project with Jacquana Smith that touches on this topic; mapping geolocation to self-reported wellbeing. 
    - A real-world, well-recognized example of this concept is the code-switching that people of color often experience when moving between spaces that are predominantly white and spaces that are predominantly BIPOC.
    - The first big steps in this are a combination of literature review and experimentation. The "Wellness in the Wild" project with Jacquana is underway, but I'm also interested in developing an arm of this research that looks at how metrics often used to measure a person's concept of self changes as a function of the physical and virtual* environments they inhabit.
> `*` Yes, I want to do a VR or Mixed Reality study on this topic!

3. What is good UX for a motion capture interface?
    - As many of you know, I'm a co-founder of [FreeMoCap](https://freemocap.org), a non-profit dedicated to making motion capture free for everyone. 
    - One of my primary roles withing FreeMoCap has been championing the usability side of things. 
    - I'd really love to develop some work that assesses the usability of the FreeMoCap system, and then use that information to improve the system.
    - I'll put the Research Question another way: *How can an interface be designed such that the affordances of the UI communicate clearly all of the core functionality of a motion capture system, in a way that is inviting and exciting for people (high school students especially) to use?*
    - You can learn about FreeMoCap by going to our [website](https://freemocap.org), or checking out our [GitHub Page](https://github.com/freemocap/freemocap).
    - In terms of a specific experiment, I'm imagining a situation where we develop a task for motion capture novices to perform, and then measure the time to completion and satisfaction using a number of different layouts. 
    - This is a **large** project, that stands to benefit me personally as a co-founder of the system. 
    - But, it's also an opportunity for you to get product-oriented applied research experience while at UC.  

4. How do people learn to use LLM Chatbots?
    - I'm teaching students how to use LLMs to build chatbots in my capstone course, and I'd like to collect data on this systematically in order to develop an understanding of how these technologies are interacted with.
    - A problem with this tech is that is is `A` incredibly new and `B` we have no clue how long chatbots, in the way they exist now, will persist. 
    - Regardless, I think it's an important question that I know many corporations are asking (there's a reason the UI and experiences of using these things are constantly changing a little), but I think it's important to do this work in a free and open way, and then communicate it to empower the open source LLM community. 
    - This is another UX research, product-oriented project - and in a weird way it could benefit me personally as well.
        1. I teach people how to use these things, if I used research to get better at it, my teaching would improve.
        2. There is an LLM project within FreeMoCap that I'm slowly getting closer to. If you want to look at [SkellyBot](https://github.com/freemocap/skellybot), go for it :) 

### Tech Tools
Technologies that I'm interested in *using for research*, I.e., this is a list of things I'm interested in digging into *methodologically*. Some I have experience with, others I do not.

- General Programming Languages
    - [Python](https://www.python.org) - I'd like to use Python for everything :)
    - [Dart](https://dart.dev) (`C#`-like) - used for Flutter Development
    - [C#](https://docs.microsoft.com/en-us/dotnet/csharp/) - used for Unity & Lightship development
- Apps
    - [Flutter](https://flutter.dev) (full-stack app development framework, by Google)
- Web
    - [JavaScript](https://www.javascript.com) (programming language, primarily used for web development)
    - [Django](https://www.djangoproject.com) (Python based web development framework)
- AR/VR
    - [Unity](https://unity.com)
    - [Unreal](https://www.unrealengine.com/en-US/)
    - [A-Frame](https://aframe.io)
    - [Lightship](https://lightship.dev/)
- Motion Capture
    - [FreeMoCap](https://freemocap.org)

